---
title: 'Multi-task + Prompt'
date: 2024-07
permalink: /posts/2024/07/08/blog
tags:
  - cool posts
  - category1
  - category2
---

📖 Paper Record
======

[2023 KDD] All in One: Multi-Task Prompting for Graph Neural Networks
------
- **KEYWORDS: pre-training; prompt tuning; graph neural networks**
- **Introduction**
  - 传统的图监督学习方法严重依赖于图标签，可能面临过拟合问题（尤其是当测试数据不分布时）。 因此考虑到使用pre-training + fine-turing的手段。 但是预训练模型的任务和具体的下游任务不一致（这里指的是node-level, edge-level and graph-level, multi-class node classification等问题，这些被视作多种（或者是类型）下游任务 multi-task）会导致在fine-tuning的时候产生负迁移问题（注意：此处的负迁移和多任务学习的负迁移不一样，这里指的负迁移问题是使用迁移学习的思想将预训练模型迁移到具体下游任务时产生的性能下降问题）。 解决这一问题，现有的方法会将pre-training + fine-turing 变化为 pre-training + prompting + fine-turing （这里指对graph的操作）。
  - prompting 来自 NLP中的 Prompt learning。NLP中的prompt是指附加在输入文本后面的一段文本，通过加一些prompt来实现新的任务，文中的例子是通过一个prompt将一个情感分析任务转换为单词预测任务，因为该模型已经通过遮蔽词所包含的情感进行了预训练，因此预测这个单词是可行的（🤯这么神奇吗）。\\
![image](https://github.com/ZeyuLiu0706/zeyuliu0706.github.io/assets/58979380/bdbbbe38-4258-4111-a158-696c609a7c95)\\
  - 图prompt和文本prompt之间的区别和所面临的问题：

- Key Contributions:
- Method
- Experiment
- Thinkings

